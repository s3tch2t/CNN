# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JuxOaxu15O8ddULbU2UVe0Y9lff8_frD
"""

# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nRqq-iwQzgdWD-_sIzgpwhS4sE4OaVij
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import plotly
import plotly.graph_objects as go
# %matplotlib inline

from sklearn.calibration import calibration_curve
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.optim import lr_scheduler

np.random.seed(42)

train = pd.read_csv('/kaggle/input/mnist-digit/train.csv', dtype=np.float32)
test = pd.read_csv('/kaggle/input/mnist-digit/test.csv', dtype=np.float32)

BATCH_SIZE = 128
EPOCHS = 8

train_tensor = torch.utils.data.TensorDataset(X_train, y_train)
val_tensor = torch.utils.data.TensorDataset(X_val, y_val)
test_tensor = torch.utils.data.TensorDataset(X_test)

train_loader = torch.utils.data.DataLoader(train_tensor,
                                           batch_size = BATCH_SIZE,
                                           shuffle = True)
val_loader = torch.utils.data.DataLoader(val_tensor,
                                         batch_size = BATCH_SIZE,
                                         shuffle = False)
test_loader = torch.utils.data.DataLoader(test_tensor,
                                          batch_size = BATCH_SIZE,
                                          shuffle = False)

model = CNNModel()

def fit(epoch):

    print("Training...")
    model.train()

    exp_lr_scheduler.step()

    train_running_loss = 0.0
    train_running_correct = 0
    train_running_lr = optimizer.param_groups[0]['lr']

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data.view(BATCH_SIZE, 1, 28, 28)), Variable(target)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        train_running_loss += loss.item()
        _, preds = torch.max(output.data, 1)
        train_running_correct += (preds == target).sum().item()

        loss.backward()
        optimizer.step()

        if (batch_idx + 1) % 50 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                 epoch + 1,
                 (batch_idx + 1) * len(data),
                 len(train_loader.dataset),
                 BATCH_SIZE * (batch_idx + 1) / len(train_loader),
                 loss.cpu().detach().numpy())
                 )

    train_loss = train_running_loss / len(train_loader.dataset)
    train_accuracy = 100. * train_running_correct / len(train_loader.dataset)

    return train_loss, train_accuracy, train_running_lr

def validate(data_loader):

    print("Validating...")

    model.eval()
    val_preds = torch.LongTensor()
    val_proba = torch.LongTensor()

    val_running_loss = 0.0
    val_running_correct = 0

    for data, target in data_loader:
        data, target = Variable(data.view(BATCH_SIZE, 1, 28, 28), volatile=False), Variable(target)

        output = model(data)
        loss = criterion(output, target)

        val_running_loss += loss.item()
        pred = output.data.max(1, keepdim=True)[1]
        proba = torch.nn.functional.softmax(output.data)

        val_running_correct += pred.eq(target.data.view_as(pred)).cpu().sum()

        val_preds = torch.cat((val_preds, pred), dim=0)
        val_proba = torch.cat((val_proba, proba))

    val_loss = val_running_loss / len(data_loader.dataset)
    val_accuracy = 100. * val_running_correct / len(data_loader.dataset)

    return val_loss, val_accuracy, val_preds, val_proba

train_loss, train_accuracy = [], []
val_loss, val_accuracy = [], []
val_preds, val_proba = [], []
train_lr = []

for epoch in range(EPOCHS):

    print(f"Epoch {epoch+1} of {EPOCHS}\n")

    train_epoch_loss, train_epoch_accuracy, train_epoch_lr = fit(epoch)
    val_epoch_loss, val_epoch_accuracy, val_epoch_preds, val_epoch_proba = validate(val_loader)

    train_loss.append(train_epoch_loss)
    train_accuracy.append(train_epoch_accuracy)
    train_lr.append(train_epoch_lr)

    val_loss.append(val_epoch_loss)
    val_accuracy.append(val_epoch_accuracy)
    val_preds.append(val_epoch_preds)
    val_proba.append(val_epoch_proba)

    print(f"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f}")
    print(f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f}\n')

def plot_history():

    plt.figure(figsize = (20,15))

    plt.subplot(221)

    plt.subplot(222)
    plt.plot(train_loss)
    plt.plot(val_loss)
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.grid()


plot_history()